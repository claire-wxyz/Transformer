{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadc22c5",
   "metadata": {},
   "source": [
    "Updated colab notebook: https://colab.research.google.com/drive/19oMDAjNZ7hj64pw91TVGv46lVOYCcqZf#scrollTo=eAi-u9COtlWJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Dense as Linear\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "\n",
    "#You only need decoder if generating text\n",
    "#just sentiment analysis = encoder --> feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.Module):\n",
    "  #input = info after positional embedding\n",
    "  def __init__(self, vocab_size, input_dim, embedding_dim, out_seq_length, n_heads=3):\n",
    "    #self.embedding = embedding layer\n",
    "    self.TextVectorization = tf.keras.layers.TextVectorization(max_tokens = vocab_size, output_mode='int', output_sequence_length=out_seq_length) \n",
    "\n",
    "    self.Encoder = Encoder(embedding_dim, n_heads=n_heads)\n",
    "    self.Decoder = Decoder(embedding_dim, n_heads=n_heads) \n",
    "\n",
    "    #final classification layers of model (see graphic if confused later)\n",
    "    #Extra dense layer might help later: output = self.Dense()\n",
    "    self.Dense = tf.keras.layers.Dense(vocab_size)\n",
    "    self.Softmax = tf.keras.layers.Softmax()\n",
    "  \n",
    "  def positional_encoding(self, matrix, n=10000):\n",
    "    empty_matrix = np.zeros(matrix.shape) #index to last 2\n",
    "    embedding_dim = matrix.shape[1]\n",
    "    for row in range(matrix.shape[0]):\n",
    "      for column in range(int(embedding_dim/2)):\n",
    "        denom = np.power(n, 2*column/embedding_dim)\n",
    "        #1st column, etc\n",
    "        empty_matrix[row, 2*column] = np.sin(row/denom)\n",
    "        #2nd column, etc.\n",
    "        empty_matrix[row, 2*column+1] = np.cos(row/denom)\n",
    "        final_matrix = matrix + empty_matrix # Can't this be added at the end and then returned?\n",
    "    print(\":)\")\n",
    "    return final_matrix\n",
    "\n",
    "  def call(self, converted_dataset):\n",
    "    model = tf.keras.Sequential([self.callTextVectorization, \n",
    "                                 self.positional_encoding(self.TextVectorization.adapt(converted_dataset)),\n",
    "                                 self.Encoder(),\n",
    "                                 self.Encoder(),\n",
    "                                 self.Encoder(),\n",
    "                                 self.Decoder(),\n",
    "                                 self.Decoder(),\n",
    "                                 self.Decoder(),\n",
    "                                 self.Decoder(),\n",
    "                                 self.Dense(),\n",
    "                                 self.Softmax()])\n",
    "\n",
    "  def old_call(self, converted_dataset):\n",
    "    vectorized_layer = self.DecoderTextVectorization\n",
    "\n",
    "    #dataset must be a tf.data.Dataset or numpy array\n",
    "    #convert dataset to tensorflow outside of Transformer structure\n",
    "    vectorized_layer.adapt(converted_dataset)\n",
    "    vectorized_layer.get_vocabulary() #can omit later, just prints vocab\n",
    "    \n",
    "    #shape after PE is finished: OG matrix\n",
    "    #self.positional_encoding = encoding layer --> sum of input embedding vector and positive vector\n",
    "    output = self.positional_encoding(vectorized_layer)\n",
    "\n",
    "    #shape after Encoder is finished: ((units * num heads), embedding_dim)\n",
    "    output = self.Encoder(output)\n",
    "    output = self.Encoder(output)\n",
    "    output = self.Encoder(output)\n",
    "\n",
    "    #shape after Decoder is finished: ((units * num heads), embedding_dim)\n",
    "    output = self.Decoder()\n",
    "    output = self.Decoder()\n",
    "    output = self.Decoder()\n",
    "\n",
    "    output = self.Dense(output)\n",
    "\n",
    "    #output probabilities = vocabulary, aka input words --> num words total, take max value\n",
    "    #put word through output embedding, and shove it back in\n",
    "    #should be size of vocab, take the highest probability --> after, turn it into an output embedding, and put it into the decoder\n",
    "    output = self.Softmax(output)\n",
    "\n",
    "#tf.Module = parent class\n",
    "#info can be passed from encoder to encoder \n",
    "class Encoder(tf.Module):\n",
    "  def __init__(self, embedding_dim, n_heads=3):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.MultiHeadedAttention = MultiHeadedAttention(embedding_dim, n_heads=n_heads)\n",
    "\n",
    "    #normalizing = transformation of data that centers data to 0, standard deviation = 1\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization\n",
    "    self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    #Dense layers = input_dim[-1] so it matches input dim\n",
    "    self.feed_forward = tf.keras.Sequential([tf.keras.layers.Dense(embedding_dim),\n",
    "                                             tf.keras.layers.ReLU(),\n",
    "                                             tf.keras.layers.Dense(embedding_dim)])\n",
    "    \n",
    "    #shape of input & output of Multiheadedattention must match, feed forward too\n",
    "    self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def __call__(self, x):\n",
    "    #calling forward from multi-head attention\n",
    "    attention_output = self.MultiHeadedAttention.forward(x)\n",
    "\n",
    "    #skip connection pathway = add & norm\n",
    "    #add to prevent vanishing + exploding gradients\n",
    "    x = self.norm1(attention_output + x)\n",
    "\n",
    "    #feed forward\n",
    "    ff_output = self.feed_forward(x)\n",
    "\n",
    "    #skip connection pathway\n",
    "    x = self.norm2(ff_output + x)\n",
    "\n",
    "    return x\n",
    "    \n",
    "\n",
    "class MultiHeadedAttention(tf.Module):\n",
    "  def __init__(self, embedding_dim, mask=False, n_heads=3):\n",
    "    super().__init__()\n",
    "    \n",
    "    #applies mask to future words if necessary\n",
    "    self.mask = mask\n",
    "    self.n_heads = n_heads\n",
    "    \n",
    "    #passing x in does matrix multiplication, multiplies x by q weight\n",
    "    self.embedding_dim = embedding_dim #size (col) of what we want final output to be\n",
    "    self.Q = [tf.keras.layers.Dense(self.embedding_dim) for i in range(n_heads)]  #use offset\n",
    "    self.K = [tf.keras.layers.Dense(self.embedding_dim) for i in range(n_heads)]\n",
    "    self.V = [tf.keras.layers.Dense(self.embedding_dim) for i in range(n_heads)]\n",
    "    #concatenation layer --> return to original shape, which is ((units * num heads), embedding_dim)\n",
    "    self.concat_layer = tf.keras.layers.Dense(self.embedding_dim)\n",
    "    self.softmax = tf.keras.layers.Softmax()\n",
    "  \n",
    "  def forward(self, x, encoder_output=None):\n",
    "    #Assuming no batches --> also assuming the ENTIRE program has no batch size, look at later\n",
    "    num_words = x.shape[0]\n",
    "\n",
    "    for i in range(self.n_heads):\n",
    "      queries = self.Q[i](x)\n",
    "      if encoder_output is not None:\n",
    "        keys = self.K[i](encoder_output)\n",
    "        values = self.V[i](encoder_output)\n",
    "      else:\n",
    "        keys = self.K[i](x)\n",
    "        values = self.V[i](x)\n",
    "\n",
    "      #https://www.tensorflow.org/api_docs/python/tf/linalg/matmul\n",
    "      query_key = tf.linalg.matmul(queries, keys.T)\n",
    "      dimension_k = (x.shape[0]*x.shape[1])\n",
    "\n",
    "      #scaled query_key\n",
    "      query_key = query_key / tf.sqrt(dimension_k)\n",
    "\n",
    "      #apply mask\n",
    "      if self.mask == True:\n",
    "        heads_output = []\n",
    "        negative_mask = np.zeros(query_key.shape)\n",
    "        for row in range(num_words-1):\n",
    "            negative_mask[row, row+1:] = -np.inf\n",
    "            #apply mask\n",
    "            query_key = query_key + negative_mask\n",
    "\n",
    "      query_key = self.softmax(query_key)\n",
    "\n",
    "      final_matrix = tf.linalg.matmul(query_key, values)\n",
    "\n",
    "      #final shape of head: (words, units), output_size is units & words = num words\n",
    "      head_shape = final_matrix * (num_words, self.embedding_dim)\n",
    "\n",
    "      #final shape: ((words, units), num_heads) = (words, (units * num heads)) \n",
    "      heads_output.append(head_shape)\n",
    "      final_heads = tf.concat(heads_output)\n",
    "    \n",
    "    #return to original shape, which is ((units * num heads), embedding_dim)\n",
    "    squashed_heads = self.concat_layer(final_heads)\n",
    "\n",
    "    return squashed_heads\n",
    "\n",
    "\n",
    "#google what is the first character input --> 1 row, embed size\n",
    "#input is 1 vector for the first time\n",
    "#input # of vectors increases by one each time it runs through the encoder/decoder\n",
    "class Decoder(tf.Module):\n",
    "  def __init__(self, embedding_dim, n_heads=3):\n",
    "    #track output embedding shape --> should be tuple (rows = words, columns = embedding size)\n",
    "    \n",
    "    #self.Attention = Attention(mask=True)\n",
    "    self.MultiHeadedAttention1 = MultiHeadedAttention(embedding_dim, mask=True, n_heads=n_heads)\n",
    "\n",
    "    #self.norm1 = Layer_norm()\n",
    "    self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    #2nd multi headed attention layer\n",
    "    self.MultiHeadedAttention2 = MultiHeadedAttention(embedding_dim, n_heads=n_heads)\n",
    "\n",
    "    #self.norm2 = Layer_norm()\n",
    "    self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    #self.feed_forward = feed forward block (linear, relu, linear)\n",
    "    self.feed_forward = tf.keras.Sequential([tf.keras.layers.Dense(embedding_dim),\n",
    "                                             tf.keras.layers.ReLU(),\n",
    "                                             tf.keras.layers.Dense(embedding_dim)])\n",
    "    \n",
    "    self.norm3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def __call__(self, previous_outputs, encoder_output):\n",
    "    #first pass: start token, 2nd pass: start token + first word\n",
    "    x = self.embedding(previous_outputs)\n",
    "    x = self.positional_encoding(x) + x\n",
    "\n",
    "    #multi-head attention\n",
    "    attention_output = self.MultiHeadedAttention1(x)\n",
    "\n",
    "    #skip connection pathway/add & norm\n",
    "    #add to prevent vanishing + exploding gradients\n",
    "    query = self.norm1(attention_output + x) #output = new query values\n",
    "\n",
    "    #x = output (no longer query)\n",
    "    x = self.MultiHeadedAttention2(query, encoder_output)\n",
    "\n",
    "    x = self.norm2(query + x)\n",
    "    \n",
    "    #feed forward\n",
    "    ff_output = self.feed_forward(x)\n",
    "\n",
    "    #skip connection pathway\n",
    "    x = self.norm3(ff_output + x)\n",
    "\n",
    "#y-data --> label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/content/drive/MyDrive/Datasets/Sentiment140/train_data.csv\", encoding='latin-1', names=['sentiment', 'date', 'no_query', 'username', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c186ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only = dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f3d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00703590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
